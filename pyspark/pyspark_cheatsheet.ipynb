{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setting up the SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load a toy csv dataset about synthetic customers for illustration\n",
    "ROOT_PATH = os.getcwd()\n",
    "INPUT_PATH = os.path.join(ROOT_PATH, \"inputs\", \"historized_customer.csv\")\n",
    "\n",
    "df = spark.read\\\n",
    "          .option(\"header\", True)\\\n",
    "          .option(\"inferSchema\", True)\\\n",
    "          .option(\"delimiter\", \";\")\\\n",
    "          .csv(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (344, 7) \n",
      "\n",
      "Nbr of partitions: 1 \n",
      "\n",
      "---- Schema ----\n",
      "root\n",
      " |-- CUSTOMER_ID: string (nullable = true)\n",
      " |-- YoB: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- AUM: integer (nullable = true)\n",
      " |-- SEGMENT: string (nullable = true)\n",
      " |-- valid_from: string (nullable = true)\n",
      " |-- valid_to: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### General information about our dataframe\n",
    "print(\"Shape: \", (df.count(), len(df.columns)), \"\\n\")\n",
    "print(\"Nbr of partitions:\", df.rdd.getNumPartitions(), \"\\n\")\n",
    "print(\"---- Schema ----\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+------+-------+----------+----------+---+\n",
      "|customer_id| yob|country|   aum|segment|valid_from|  valid_to|age|\n",
      "+-----------+----+-------+------+-------+----------+----------+---+\n",
      "|    CUST001|1954|     UK|150000|     S1|2019-06-01|9999-12-12| 67|\n",
      "|    CUST001|1954|     FR|160000|     S1|2014-12-20|2019-06-01| 67|\n",
      "|    CUST001|1954|     CH|170000|     S1|2000-01-01|2014-12-20| 67|\n",
      "|    CUST002|1955|     UK|180000|     S1|1996-06-01|9999-12-12| 66|\n",
      "|    CUST003|1956|     UK|190000|     S1|1996-06-02|9999-12-12| 65|\n",
      "+-----------+----+-------+------+-------+----------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lower case all clolumn names and convert dates column to datetype\n",
    "df = df.select([F.col(c).alias(c.lower()) for c in df.columns])\n",
    "\n",
    "df = df.withColumn(\"valid_from\", F.to_date(F.col(\"valid_from\"), \"dd.MM.yyyy\"))\\\n",
    "       .withColumn(\"valid_to\", F.to_date(F.col(\"valid_to\"), \"dd.MM.yyyy\"))\n",
    "\n",
    "# Let's also compute their age based on the Year-of-Birth (YOB)\n",
    "df = df.withColumn(\"age\", F.lit(2021) - F.col(\"yob\"))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+------+-------+----------+----------+---+\n",
      "|customer_id| yob|country|   aum|segment|valid_from|  valid_to|age|\n",
      "+-----------+----+-------+------+-------+----------+----------+---+\n",
      "|    CUST001|1954|     UK|150000|     S1|2019-06-01|9999-12-12| 67|\n",
      "|    CUST002|1955|     UK|180000|     S1|1996-06-01|9999-12-12| 66|\n",
      "|    CUST003|1956|     UK|190000|     S1|1996-06-02|9999-12-12| 65|\n",
      "|    CUST004|1957|     UK|200000|     S1|1996-06-03|9999-12-12| 64|\n",
      "|    CUST005|1958|     UK|210000|     S1|1996-06-04|9999-12-12| 63|\n",
      "+-----------+----+-------+------+-------+----------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Deduplication with a Window-filter\n",
    "\n",
    "# Note: depending on the use case, one may need F.rank() or F.dense_rank() instead of F.row_number()\n",
    "def dedup_funk(table):\n",
    "    PARTITION = [\"customer_id\"]\n",
    "    w = Window.partitionBy(PARTITION)\\\n",
    "              .orderBy(F.col(\"valid_from\").desc(), F.col(\"valid_to\").desc())\n",
    "    \n",
    "    return table.select(\"*\", F.row_number().over(w).alias(\"rn\"))\\\n",
    "                .filter(F.col(\"rn\") == 1)\\\n",
    "                .drop(\"rn\")\n",
    "\n",
    "# Show the output example ordered by customer_ids in ascending order\n",
    "df_deduped = dedup_funk(df).orderBy(\"customer_id\")\n",
    "df_deduped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+----------+\n",
      "|country|S1_sum_aum|S1_sum_age|S2_sum_aum|S2_sum_age|\n",
      "+-------+----------+----------+----------+----------+\n",
      "|     NL|    230000|        61|      null|      null|\n",
      "|     MX|      null|      null| 321950000|      3994|\n",
      "|     ES|  14490000|       336|      null|      null|\n",
      "|     FR|    160000|        67|  37830000|      1079|\n",
      "|     CH|   9410000|       928|      null|      null|\n",
      "|     UK|  42520000|      2405| 214970000|      2066|\n",
      "+-------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Pivot with multi aggregation\n",
    "\n",
    "# It is more efficient to specify the values in the pivot function, so that Spark doesn't need\n",
    "# to first compute the list of distinct values internally.\n",
    "\n",
    "def pivot_funk(table):\n",
    "    AGGR = [F.sum(F.col(c)).alias(\"sum_\" + c) for c in [\"aum\", \"age\"]]\n",
    "    return table.groupBy(\"country\")\\\n",
    "                .pivot(\"segment\", [\"S1\", \"S2\"])\\\n",
    "                .agg(*AGGR)\n",
    "\n",
    "\n",
    "df_pivot = pivot_funk(df)\n",
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+-------+-------+----------+----------+---+------------------+\n",
      "|customer_id| yob|country|    aum|segment|valid_from|  valid_to|age|   avg_per_country|\n",
      "+-----------+----+-------+-------+-------+----------+----------+---+------------------+\n",
      "|    CUST007|1960|     NL| 230000|     S1|2018-07-08|9999-12-12| 61|              61.0|\n",
      "|    CUST150|1999|     MX|1670000|     S2|1996-10-27|9999-12-12| 22|29.153284671532848|\n",
      "|    CUST151|2000|     MX|1680000|     S2|1996-10-28|9999-12-12| 21|29.153284671532848|\n",
      "|    CUST152|2001|     MX|1690000|     S2|1996-10-29|9999-12-12| 20|29.153284671532848|\n",
      "|    CUST153|2002|     MX|1700000|     S2|1996-10-30|9999-12-12| 19|29.153284671532848|\n",
      "|    CUST154|2003|     MX|1710000|     S2|1996-10-31|9999-12-12| 18|29.153284671532848|\n",
      "|    CUST155|2004|     MX|1720000|     S2|1996-11-01|9999-12-12| 17|29.153284671532848|\n",
      "|    CUST156|2005|     MX|1730000|     S2|1996-11-02|9999-12-12| 16|29.153284671532848|\n",
      "|    CUST157|2006|     MX|1740000|     S2|1996-11-03|9999-12-12| 15|29.153284671532848|\n",
      "|    CUST158|2007|     MX|1750000|     S2|1996-11-04|9999-12-12| 14|29.153284671532848|\n",
      "+-----------+----+-------+-------+-------+----------+----------+---+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window function to compute average age per country\n",
    "def avg_age_funk(table):\n",
    "    PARTITION = [\"country\"]\n",
    "    w = Window.partitionBy(PARTITION)\n",
    "    return table.select(\"*\", F.avg(F.col(\"age\")).over(w).alias(\"avg_per_country\"))\n",
    "\n",
    "df_avg_age = avg_age_funk(df)\n",
    "df_avg_age.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Broadcast small table in a join to speed up the process\n",
    "def fast_join(big_table, small_table):\n",
    "    return big_table.join(\n",
    "        F.broadcast(small_table).coalesce(1),\n",
    "        \"PK\",\n",
    "        \"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----+-------------------+--------------------+\n",
      "|Firstname|Lastname| Age|   array_of_strings|           SWIFT_msg|\n",
      "+---------+--------+----+-------------------+--------------------+\n",
      "|    James|    Bond|  43|      [chat, chien]|                null|\n",
      "|   Pierre|     Bin|  54|[pizza, milk, null]|32A: 1700.45, 52E...|\n",
      "|     Lara|   Tempo|null|           [US, FR]|32A: 1700.45, 52E...|\n",
      "+---------+--------+----+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Create a table form scratch\n",
    "def create_df():\n",
    "    my_schema = T.StructType([\n",
    "            T.StructField(\"Firstname\", T.StringType()),\n",
    "            T.StructField(\"Lastname\", T.StringType()),\n",
    "            T.StructField(\"Age\", T.IntegerType()),\n",
    "            T.StructField(\"array_of_strings\", T.ArrayType(T.StringType())),\n",
    "            T.StructField(\"SWIFT_msg\", T.StringType()),\n",
    "        ])\n",
    "\n",
    "    my_data = [\n",
    "        {\"Firstname\": \"James\", \"Lastname\": \"Bond\", \"Age\": 43, \"array_of_strings\": [\"chat\", \"chien\"], \"SWIFT_msg\": None},\n",
    "        {\"Firstname\": \"Pierre\", \"Lastname\": \"Bin\", \"Age\": 54, \"array_of_strings\": [\"pizza\", \"milk\", None], \"SWIFT_msg\": \"32A: 1700.45, 52E: CHF\"},\n",
    "        {\"Firstname\": \"Lara\", \"Lastname\": \"Tempo\", \"Age\": None, \"array_of_strings\": [\"US\", \"FR\"], \"SWIFT_msg\": \"32A: 1700.45, 52E: CHF, 70: London\"}\n",
    "    ]\n",
    "\n",
    "    return spark.createDataFrame(my_data, my_schema)\n",
    "\n",
    "\n",
    "my_df = create_df()\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----+-------------------+--------------------+--------------+--------------+\n",
      "|Firstname|Lastname| Age|   array_of_strings|           SWIFT_msg|Amount_method1|Amount_method2|\n",
      "+---------+--------+----+-------------------+--------------------+--------------+--------------+\n",
      "|    James|    Bond|  43|      [chat, chien]|                null|          null|          null|\n",
      "|   Pierre|     Bin|  54|[pizza, milk, null]|32A: 1700.45, 52E...|       1700.45|       1700.45|\n",
      "|     Lara|   Tempo|null|           [US, FR]|32A: 1700.45, 52E...|       1700.45|       1700.45|\n",
      "+---------+--------+----+-------------------+--------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Parsing\n",
    "\n",
    "# Let's try to get the amount in two different ways\n",
    "# The first approach uses \"split\" and getItem functions\n",
    "# The second approach uses regex extraction\n",
    "def parser(table):\n",
    "    return table.withColumn(\"Amount_method1\", F.split(F.col(\"SWIFT_MSG\"), \"32A:\").getItem(1))\\\n",
    "                .withColumn(\"Amount_method1\", F.split(F.col(\"Amount_method1\"), \", 52E:\").getItem(0))\\\n",
    "                .withColumn(\"Amount_method2\", F.regexp_extract('SWIFT_MSG', r'(32A:\\s)([0-9]*\\.[0-9]{1,})', 2))\n",
    "\n",
    "\n",
    "parse_df = parser(my_df)\n",
    "parse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Join on levenstein distance condition\n",
    "def special_cond_join(table1, table2):\n",
    "    join_cond = (\n",
    "        (F.levenshtein(table1[\"Firstname\"], table2[\"prenom\"]) < 5)\n",
    "        &\n",
    "        (table1[\"years_old\"] == table2[\"age\"])\n",
    "    )\n",
    "    return table1.join(table2, join_cond, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### String cleaner function using regex\n",
    "def cleaner(table, list_cols):\n",
    "    # remove block spaces bigger than 1 and trim\n",
    "    for c in list_cols:\n",
    "        table = table.withColumn(\"cleaned_\" + c, F.trim(F.regexp_replace(c, r'(\\s{2,})', '')))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------+\n",
      "|prenom|nom_de_famille|new_age|\n",
      "+------+--------------+-------+\n",
      "| James|          Bond|     45|\n",
      "|Pierre|           Bin|     56|\n",
      "|  Lara|         Tempo|   null|\n",
      "+------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Mapping function using SQL like statements\n",
    "def mapper(table):\n",
    "    my_mapings = [\n",
    "        \"Firstname as prenom\",\n",
    "        \"Lastname as nom_de_famille\",\n",
    "        \"Age + 2 as new_age\"\n",
    "    ]\n",
    "    return table.selectExpr(my_mapings)\n",
    "\n",
    "\n",
    "map_df = mapper(my_df)\n",
    "map_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
